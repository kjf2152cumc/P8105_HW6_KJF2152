---
title: "P8105 HW 6"
author: "Kaleb J. Frierson"
date: "2024-11-23"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Homework Introduction

This assignment from Data Science I draws on the Linear Models unit. This includes the linear models, cross validation, and bootstrapping lectures. Necessary data has been moved into the working directory of this repository saved R project. Formatting has been carried out per guidelines for problem 0.

## Library Calling

Below all needed libraries are loaded:
```{r libraries, message = FALSE}
library(tidyverse)
library(broom)
library(modelr)
library(knitr)
library(purrr)
```


# Problem 1

The below code chunk below adapted from the course website download's Central Park weather data from 2017:
```{r noaa_data, message=FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Create `bootstrap_1` `function` that returns a `tibble` with values of r2, b0, and b1:
```{r}
bootstrap_1 = 
  
  function(df) {
  
    boot_df = sample_frac(df, replace = TRUE)
    
    boot_model = lm(tmax ~ tmin, data = boot_df)
    
    r_squared = glance(boot_model)$r.squared
    
    coefs = tidy(boot_model)
    
    log_beta_product = log(coefs$estimate[1] * coefs$estimate[2])
  
  return(tibble(r_squared = r_squared, log_beta_product = log_beta_product))
}
```

Use `map_dfr` to apply the `function` 5000 times: 
```{r apply f(bootstrap)}
output_bs1 = map_dfr(1:5000, ~bootstrap_1(weather_df))

```

`ggplot` r2: 
```{r visualize_r2}
r2plot = 
  output_bs1 |> 
  ggplot(aes(x = r_squared)) +
  geom_density(fill = "skyblue", alpha = 0.5) +
  labs(
    title = "Bootstrap Distribution of R-squared", 
    x = "R-squared", 
    y = "Density")

r2plot

```

`ggplot` log(b0 * b0): 
```{r visualize_ln(b*b)}
logplot = 
  output_bs1 |>
  ggplot(aes(x = log_beta_product)) +
  geom_density(fill = "salmon", alpha = 0.5) +
  labs(
    title = "Bootstrap Distribution of log(beta0 * beta1)", 
    x = "log(beta0 * beta1)", 
    y = "Density")

logplot

```

Compute point estimates and CIs using `summarize` and `knitr:kalble`:
```{r display results}
bootstrap_summary = 
  output_bs1 |> 
  summarize(
    r_squared_mean = mean(r_squared),
    r_squared_ci_ll = quantile(r_squared, 0.025),
    r_squared_ci_ul = quantile(r_squared, 0.975),
    log_beta_product_mean = mean(log_beta_product),
    log_beta_product_ci_ll = quantile(log_beta_product, 0.025),
    log_beta_product_ci_ul = quantile(log_beta_product, 0.975)
  )|> 
  pivot_longer(
    cols = everything(),
    names_to = c("metric", ".value"),
    names_pattern = "(.*)_(mean|ci_ll|ci_ul)"
)

bootstrap_summary |> 
  kable(
    digits = 3,               
    col.names = c("Estimate", "Mean", "CI Lower", "CI Upper"),
    caption = "Bootstrap Summary Table"
  )

```

**Describe distribution of estimates in words**: 

The distribution of r2 estimates is approximately normal with a mean of 0.912	(95% CI 0.894, 0.927). The distribution of log(b0*b1) estimates is approximately normal with a mean of 2.013	(95% CI 1.967, 2.058). 

# Problem 2

Below I read in homicide data from TWJ using `read_csv`. Note that age is not numeric. 
```{r}
homicides = 
  read_csv(
    "local data/homicide-data.csv",
    na = c("Unknown", "NA", "", ".")) |> 
  janitor::clean_names() 

spec(homicides)

```
Use `mutate` to create a city_state variable (e.g. “Baltimore, MD”), and a binary variable indicating whether the homicide is solved. Use `filter` to Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t report victim race. Also omit Tulsa, AL – this is a data entry mistake. Limit your analysis those for whom `victim_race` is white or black. Be sure that `victim_age` is treated `as.numeric`: 
```{r}
homicides = 
  homicides |> 
  mutate(
    city_state = paste(city, state, sep = ", "),
    solved = if_else(disposition == "Closed by arrest", 1, 0),
    victim_age = as.numeric(victim_age)
  ) |> 
  filter(
    !city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"),
    victim_race %in% c("White", "Black")
  )
```

For the city of Baltimore, MD, use the `glm` function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. Save the output of `glm` as an `R object`; apply the `broom::tidy` to this object; and obtain the estimate and confidence interval of the adjusted odds ratio (by exponentiation of the estimate, `exponentiate = TRUE`) for solving homicides comparing male victims to female victims keeping all other variables fixed.
```{r}
baltimore_df = 
  homicides |> 
  filter(city_state == "Baltimore, MD")

baltimore_model = 
  glm(
  solved ~ victim_age + victim_sex + victim_race,
  data = baltimore_df,
  family = "binomial"
)

baltimore_results = 
  broom::tidy(baltimore_model, conf.int = TRUE, exponentiate = TRUE) |> 
  filter(term == "victim_sexMale") |> 
  select(estimate, conf.low, conf.high)

baltimore_results |> 
  kable(
    digits = 2,               
    col.names = c("OR Estimate", "CI Lower", "CI Upper")
  )

```

Run `glm` for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. Do this within a “tidy” pipeline, making use of `purrr::map`, list columns, and `unnest` as necessary to create a dataframe with estimated ORs and CIs for each city:
```{r}
city_results = 
  homicides |> 
  group_by(city_state) |> 
  nest() |> 
  mutate(
    model = 
      map(data, ~ glm(solved ~ victim_age + victim_sex + victim_race, 
          data = .x, family = "binomial")),
    results = 
      map(model, ~ broom::tidy(.x, conf.int = TRUE, exponentiate = TRUE))
  ) |> 
  unnest(results) |> 
  filter(term == "victim_sexMale") |> 
  select(city_state, estimate, conf.low, conf.high)

city_results |> 
  kable(
    digits = 3,               
    col.names = c("City, State", "OR Estimate", "CI Lower", "CI Upper"),
    caption = "All City Regression Results"
  )

```

Use `ggplot` to show the estimated ORs and CIs for each city. Organize cities according to estimated OR:
```{r}
city_results = 
  city_results |> 
  mutate(city_state = fct_reorder(city_state, estimate))

city_results |>
  ggplot(aes(x = estimate, y = city_state)) +
  geom_point() +
  geom_errorbarh(
    aes(xmin = conf.low, xmax = conf.high), height = 0.2) +
  labs(
    title = "Adjusted Odds Ratios for Solving Homicides (Male vs Female)",
    x = "Adjusted Odds Ratio (Male vs Female)",
    y = "City"
  ) +
  theme_minimal()

```


**Comment on the plot**: the adjusted OR comparing odds of a male homicide being solved to a female homicide being solved when controlling for age, sex, and race is higher than 1 in Fresno, CA; Stockton, CA; Nashville, TN; and Albuquerque, NM. It is equal to 1 in Richmond, VA; and Atlanta, GA. In the rest of the cities it is less than 1. The confidence intervals are all very wide and often cross the null. However, on average it seems that men are less likely than women to have their homicide solved. For the reader, plot readability is best in your browser and not in R studio's platform. 

# Problem 3

Bring in the birtweight data and `mutate` it such that variables coded as 1, 2, etc aren't treated as continuous: 
```{r}
bwt_df = 
  read_csv("local data/birthweight.csv")

spec(bwt_df)

bwt_df = 
  bwt_df |> 
  mutate(
    babysex = factor(babysex, levels = c(1, 2), 
                     labels = c("Male", "Female")),
    frace = factor(frace),
    mrace = factor(mrace),
    malform = factor(malform),
    smoken = as.numeric(smoken)
  ) |> 
  drop_na()
```

Propose a regression model for birthweight. This model may be based on a hypothesized structure for the factors that underly birthweight:
```{r}
bwt_model1 = 
  lm(bwt ~ gaweeks + 
       ppbmi + 
       smoken + 
       parity + 
       malform +
       pnumlbw,
     data = bwt_df)
```


**Describe your modeling process**: I looked for variables that I don't suspect to increase multi-colinearity. For example, I know from previous knowledge that birthweight is correlated with a child's dimensions and with the mother's weight at delivery so I chose not to use those variables. I was curious about the effects of previous medical problems and risk factors so I chose to regress `bwt` on `gaweeks`, controlling for `smoken`, `parity`, `malform`, `pnumlbw`. But Kaleb, isn't there a clear relationship between gestational age and weight? Yes, but that relationship is important clinically and indicative of medical conditions and thus I think a meaningful predictor to test. 

Use `ggplot` to show residuals graphed against fitted values Use `add_predictions` and `add_residuals` in making this plot:
```{r}
bwt_df = 
  bwt_df |> 
  add_predictions(bwt_model1) |> 
  add_residuals(bwt_model1)

bwt_df |> 
ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, 
             color = "red", 
             linetype = "dashed") +
  labs(
    title = "Residuals vs Fitted Values",
    x = "Fitted Values",
    y = "Residuals"
  ) +
  theme_minimal()

```


**Comment on plot of residuals v fitted values**: Hmmmm... I think linearity is met. Not entirely sure on homoscedasticity... 


Compare the first model to one using length at birth and gestational age as predictors (main effects only). Also compare it to one using head circumference, length, sex, and all interactions (including the three-way interaction) between these. Use `crossv_mc` and functions in `purrr` as appropriate: 
```{r}
cv_df = 
  crossv_mc(bwt_df, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

cv_res_df = 
  cv_df |> 
  mutate(
    hypothesized_mod = 
      map(train, ~ lm(bwt ~ gaweeks + 
       ppbmi + 
       smoken + 
       parity + 
       malform +
       pnumlbw, data = .)),
    model_1 = 
      map(train, ~ lm(bwt ~ blength + gaweeks, data = .)),
    model_2 = 
      map(train, ~ lm(bwt ~ bhead +
                        blength + 
                        babysex +
                        bhead*blength + 
                        bhead*babysex +
                        blength*babysex, data = .))
  ) |> 
  mutate(
    rmse_hypothesized = map2_dbl(hypothesized_mod, test, rmse),
    rmse_model_1 = map2_dbl(model_1, test, rmse),
    rmse_model_2 = map2_dbl(model_2, test, rmse)
  )

cv_summary = 
  cv_res_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_"
  )

cv_summary |> 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin() +
  labs(
    title = "Model Comparison via RMSE",
    x = "Model",
    y = "RMSE"
  )

```


**Comments**: My model was worse than the other models (RMSE > 400). This might be due to outliars seen in the plot of fitted values and residuals. I also included more variables which increases opportunities for error. Model 1 has higher root mean square error than model 2 but model 2 gives a warning `prediction from rank-deficient fit`. This could be due to too many colinear variables or too many predictors for too few observations (I don't think the latter is as likely as the former). 

